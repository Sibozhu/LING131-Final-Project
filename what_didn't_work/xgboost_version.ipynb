{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "from kaggle.competitions import twosigmanews #Comment out here because this dataset is exclusive in Kaggle kernel\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'twosigmanews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-75577bd536b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwosigmanews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmarket_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"one\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'twosigmanews' is not defined"
     ]
    }
   ],
   "source": [
    "env = twosigmanews.make_env()\n",
    "(market_train, news_train) = env.get_training_data()\n",
    "gc.enable()\n",
    "print(\"one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "9e7e45eab0af5570ca16126c8acab76e14fd090d"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a775f946cc3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# index = news_train['time'][news_train['time'].dt.hour > 22].index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# news_train.loc[index,'time']  = news_train.loc[index,'time'].dt.ceil('d')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnews_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m cols = ['sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize','sentenceCount','wordCount','firstMentionSentence',\n\u001b[0;32m      5\u001b[0m         \u001b[1;34m'sentimentWordCount'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'takeSequence'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'sentimentClass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'noveltyCount12H'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'noveltyCount24H'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'noveltyCount3D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'noveltyCount5D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'noveltyCount7D'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4366\u001b[0m         if (name in self._internal_names_set or name in self._metadata or\n\u001b[0;32m   4367\u001b[0m                 name in self._accessors):\n\u001b[1;32m-> 4368\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4369\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4370\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0maccessor_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[1;32mpass\u001b[0m  \u001b[1;31m# we raise an attribute error anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         raise AttributeError(\"Can only use .dt accessor with datetimelike \"\n\u001b[0m\u001b[0;32m    326\u001b[0m                              \"values\")\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# index = news_train['time'][news_train['time'].dt.hour > 22].index\n",
    "# news_train.loc[index,'time']  = news_train.loc[index,'time'].dt.ceil('d')\n",
    "news_train['time'] = news_train['time'].dt.floor('d')\n",
    "cols = ['sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize','sentenceCount','wordCount','firstMentionSentence',\n",
    "        'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', 'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', \n",
    "        'volumeCounts12H','volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D','volumeCounts7D']\n",
    "\n",
    "news_total = news_train[['time','assetName'] + cols].copy()\n",
    "del news_train\n",
    "gc.collect()\n",
    "news_train = news_total\n",
    "print(news_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04858b97eb857adcc9f398fbc65535b1cb6340b9"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action ='ignore',category = DeprecationWarning)\n",
    "\n",
    "#directly multiplication would cause worse results than 0.66972, for now we don't do multiplication\n",
    "# for col in cols:\n",
    "#     if col != 'relevance':\n",
    "#         print(col)\n",
    "#         news_train[col] = news_train[col] * news_train['relevance']\n",
    "#get all the day's post three day's news data which influences the stock market\n",
    "#executing join of news data\n",
    "def get_news_train(raw_data,days = 5):\n",
    "    news_last = pd.DataFrame()\n",
    "    rate = 1.0\n",
    "    for i in range(days):\n",
    "        cur_train = raw_data[cols] * rate \n",
    "        rate *= 0.7\n",
    "        cur_train['time'] = raw_data['time'] + datetime.timedelta(days = i,hours=22)\n",
    "        cur_train['key'] = cur_train['time'].astype(str)+ raw_data['assetName'].astype(str)\n",
    "        cur_train = cur_train[['key'] + cols].groupby('key').sum()\n",
    "        cur_train['key'] = cur_train.index.values\n",
    "        news_last = pd.concat([news_last, cur_train[['key'] + cols]])\n",
    "        del cur_train\n",
    "        gc.collect()\n",
    "        print(\"after concat the shape is:\",news_last.shape)\n",
    "        news_last = news_last.groupby('key').sum()\n",
    "        news_last['key'] = news_last.index.values\n",
    "        print(\"the result shape is:\",news_last.shape)\n",
    "       \n",
    "    del news_last['key']\n",
    "    return news_last\n",
    "\n",
    "news_last = get_news_train(news_train)\n",
    "print(news_last.shape)\n",
    "print(news_last.head())\n",
    "print(news_last.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f26fb818749b3de15657d8b73cd4bb75b1227e2"
   },
   "outputs": [],
   "source": [
    "market_train['key'] = market_train['time'].astype(str) + market_train['assetName'].astype(str)\n",
    "market_train = market_train.join(news_last,on = 'key',how='left')\n",
    "print(market_train['sentimentNeutral'].isnull().value_counts())\n",
    "market_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0daf8923ed239a73cb70fc631662cfcea1fd06cb"
   },
   "outputs": [],
   "source": [
    "# print(market_train['assetName'].nunique())\n",
    "# print(news_train['assetName'].nunique())\n",
    "# from assetName we can see there are over 120,000 examples not showing up in news matching the market data\n",
    "# print(market_train['assetName'].isin(news_train['assetName']).value_counts())\n",
    "# print(market_train['time'].nunique())\n",
    "# print(news_train['time'].nunique())\n",
    "# print(news_train['time'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5dfa1843dfece6fccfca91896ef85a332b55e3e6"
   },
   "outputs": [],
   "source": [
    "cat_cols = ['assetCode','assetName']\n",
    "num_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n",
    "                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n",
    "                    'returnsOpenPrevMktres10','sentimentNegative','sentimentNeutral','sentimentPositive','relevance','companyCount','bodySize',\n",
    "            'sentenceCount','wordCount','firstMentionSentence', 'sentimentWordCount','takeSequence','sentimentClass','noveltyCount12H', \n",
    "            'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H','volumeCounts24H', 'volumeCounts3D',\n",
    "            'volumeCounts5D','volumeCounts7D']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e108339134e95473b4a983237d58adb64c3ef64a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51d00dc43857b446ae4a24b3718753f09040fd5"
   },
   "source": [
    "# Handling categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "301a65b834d8614a914883d49be1860550174f06"
   },
   "outputs": [],
   "source": [
    "def encode(encoder, x):\n",
    "    len_encoder = len(encoder)\n",
    "    try:\n",
    "        id = encoder[x]\n",
    "    except KeyError:\n",
    "        id = len_encoder\n",
    "    return id\n",
    "encoders = [{} for i in range(len(cat_cols))]\n",
    "for i, cat in enumerate(cat_cols):\n",
    "    print('encoding %s ...' % cat, end=' ')\n",
    "    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].unique())}\n",
    "    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n",
    "    print('Done')\n",
    "\n",
    "embed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "397360cbacbcb294daf5e0750f471e13ced31978"
   },
   "source": [
    "# Handling numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e6c878e412b3e819e056cee40181b96fbac2f78"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "import matplotlib\n",
    "# market_train[num_cols] = market_train[num_cols].fillna(0)\n",
    "# filtering the deviant point\n",
    "# print(market_train['close'][market_train['close'] > 1000].count())\n",
    "# print(market_train['open'][market_train['open'] > 1000].count())\n",
    "# print(market_train['volume'][market_train['volume'] > 1e+08].count())\n",
    "\n",
    "market_train['close'].clip(upper = 1000, inplace = True)\n",
    "market_train['open'].clip(upper = 1000, inplace = True)\n",
    "market_train['volume'].clip(upper = 1e+08, inplace = True)\n",
    "\n",
    "# matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "# prices = pd.DataFrame({\"close\":market_train[\"close\"], \"log(close + 1)\":np.log1p(market_train[\"close\"])})\n",
    "# prices.hist(bins = 10)\n",
    "# market_train['close'] = np.log1p(market_train['close'])\n",
    "# market_train['open'] = np.log1p(market_train['open'])\n",
    "\n",
    "print('scaling numerical columns')\n",
    "scaler = StandardScaler()\n",
    "col_mean = market_train[num_cols].mean()\n",
    "market_train[num_cols]=market_train[num_cols].fillna(col_mean)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "market_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n",
    "# market_train.describe()\n",
    "# market_train[num_cols].isna()\n",
    "# market_train['returnsClosePrevMktres1'].isnull().value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eae09951757845ad5bdf08e004e6477513ed739a"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb217fc475f418f18720f87a0596aa23eddeb209"
   },
   "outputs": [],
   "source": [
    "def get_input(market_train, indices):\n",
    "    X = market_train.loc[indices, num_cols]\n",
    "    for cat in cat_cols:\n",
    "        X[cat] = market_train.loc[indices, cat].values\n",
    "    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n",
    "    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n",
    "    u = market_train.loc[indices, 'universe']\n",
    "    d = market_train.loc[indices, 'time'].dt.date\n",
    "    return X,y,r,u,d\n",
    "\n",
    "# r, u and d are used to calculate the scoring metric\n",
    "X_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\n",
    "\n",
    "X_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)\n",
    "X_train.shape\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecd18fe6d1a0ecf20678f94d7ba4746a7103b407"
   },
   "source": [
    "# Train  model using hyperopt to auto hyper_parameters turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "542f77f1f5675067d810614da9add097266b85a5"
   },
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "from functools import partial\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "algo = partial(tpe.suggest, n_startup_jobs=10)\n",
    "def auto_turing(args):\n",
    "    #model = XGBClassifier(n_jobs = 4, n_estimators = args['n_estimators'],max_depth=6)\n",
    "    model = lgb.LGBMClassifier(n_estimators=args['n_estimators'])\n",
    "    model.fit(X_train,y_train.astype(int))\n",
    "    confidence_valid = model.predict(X_valid)*2 -1\n",
    "    score = accuracy_score(confidence_valid>0,y_valid)\n",
    "    print(args,score)\n",
    "    return -score\n",
    "space = {\"n_estimators\":hp.choice(\"n_estimators\",range(20,200))}\n",
    "print(fmin)\n",
    "best = fmin(auto_turing, space, algo=algo,max_evals=30)\n",
    "print(best)\n",
    "\n",
    "# xgboost part\n",
    "model = XGBClassifier(n_jobs = 4, n_estimators = 80, max_depth=6, subsample = 0.66,colsample_bytree = 0.66,learning_rate = 0.1)\n",
    "model.fit(X_train,y_train.astype(int))\n",
    "confidence_valid = model.predict(X_valid)*2 -1\n",
    "score = accuracy_score(confidence_valid>0,y_valid)\n",
    "print(score)\n",
    "print(\"MSE\")\n",
    "print(mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n",
    "# lightGBM part, faster than xgboost\n",
    "# import lightgbm as lgb\n",
    "# model = lgb.LGBMClassifier(num_threads = 4, n_estimators=100, feature_fraction = 0.66, bagging_fraction = 0.66,\n",
    "#                            early_stopping_rounds = 10,valid_sets = [X_valid, y_valid.astype(int)],objective = 'binary', metric='binary_logloss')\n",
    "# model.fit(X_train,y_train.astype(int))\n",
    "# confidence_valid = model.predict(X_valid)*2 -1\n",
    "# score = accuracy_score(confidence_valid>0,y_valid)\n",
    "# print(score)\n",
    "# print(\"MSE\",mean_squared_error(confidence_valid > 0, y_valid.astype(float)))\n",
    "\n",
    "# custom function to run light gbm model\n",
    "# def run_lgb(train_X, train_y, val_X, val_y,args):\n",
    "#     params = {\n",
    "#         \"objective\" : \"binary\",\n",
    "#         \"metric\" : \"binary_logloss\", \n",
    "#         \"num_leaves\" : args['num_leaves'],\n",
    "#         \"min_child_samples\" : args['min_child_samples'],\n",
    "#         \"learning_rate\" : args['learning_rate'],\n",
    "#         \"bagging_fraction\" : 0.7,\n",
    "#         \"feature_fraction\" : 0.66,\n",
    "#         \"bagging_frequency\" : 5,\n",
    "#         \"bagging_seed\" : 2018,\n",
    "#         \"verbosity\" : -1\n",
    "#     }\n",
    "    \n",
    "#     lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "#     lgval = lgb.Dataset(val_X, label=val_y)\n",
    "#     model = lgb.train(params, lgtrain, args['n_estimators'], valid_sets=[lgval], early_stopping_rounds=50, verbose_eval=100)\n",
    "    \n",
    "#     pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_val_y = model.predict(val_X, num_iteration=model.best_iteration)\n",
    "    confidence_valid = model.predict(X_valid)*2 -1\n",
    "    score = accuracy_score(confidence_valid > 0 , y_valid)\n",
    "    print(score)\n",
    "    mse = mean_squared_error(confidence_valid > 0, y_valid.astype(float))\n",
    "    print(\"MSE\", mse)\n",
    "    print(\"args\",args)\n",
    "    return model, mse\n",
    "\n",
    "# def auto_turing(args):\n",
    "#     model, mse = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int),args)\n",
    "#     return mse\n",
    "# space = {\"n_estimators\":hp.choice('n_estimators',range(100,1000)),\n",
    "#          \"num_leaves\":hp.choice('num_leaves',range(20,100)),\n",
    "#          \"min_child_samples\":hp.choice(\"min_child_samples\",range(20,2000)),\n",
    "#          'learning_rate':hp.loguniform('learning_rate',0.01,0.3),\n",
    "#          'max_depth': hp.choice('max_depth', range(3,8))\n",
    "#         }\n",
    "# print(fmin)\n",
    "# best = fmin(auto_turing, space, algo=algo,max_evals=100)\n",
    "# print(best)\n",
    "args = {'learning_rate': 1.0958730495793214, 'max_depth': 7, 'min_child_samples': 301, 'n_estimators': 439, 'num_leaves': 43}\n",
    "model, _ = run_lgb(X_train, y_train.astype(int), X_valid, y_valid.astype(int), args)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# distribution of confidence that will be used as submission\n",
    "# plt.hist(confidence_valid, bins='auto')\n",
    "# plt.title(\"predicted confidence\")\n",
    "# plt.show()\n",
    "# these are tuned params I found\n",
    "gc.collect()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bcb2cb97ac4f6b797e9fe9f3cb2682090b1e6850"
   },
   "source": [
    "Result validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d8097adcabf5794f04e4ec1f55f2993ef9176cb"
   },
   "outputs": [],
   "source": [
    "# calculation of actual metric that is used to calculate final score\n",
    "confidence_valid = model.predict(X_valid)*2 -1\n",
    "r_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\n",
    "x_t_i = confidence_valid * r_valid * u_valid\n",
    "data = {'day' : d_valid, 'x_t_i' : x_t_i}\n",
    "df = pd.DataFrame(data)\n",
    "x_t = df.groupby('day').sum().values.flatten()\n",
    "mean = np.mean(x_t)\n",
    "std = np.std(x_t)\n",
    "score_valid = mean / std\n",
    "print(score_valid)\n",
    "market_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a556452e64bef6bee50b7e281ef65f2923c554a7"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f57d1b1d8cf8bea0594e0d699f1b849566ebad86"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cea4e659153962bdf2062b0ca10943927549a26"
   },
   "outputs": [],
   "source": [
    "n_days = 0\n",
    "predicted_confidences = np.array([])\n",
    "from collections import deque\n",
    "news_pre = deque()\n",
    "news_all = pd.DataFrame()\n",
    "BaseMod = 50\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    n_days +=1\n",
    "    print(n_days,end=' ')\n",
    "    news_all = pd.concat([news_all,news_obs_df])\n",
    "    if n_days >= BaseMod and n_days % BaseMod >= 0 and n_days % BaseMod < 8:\n",
    "        news_pre.append(news_obs_df)\n",
    "    elif n_days >= BaseMod and n_days % BaseMod == 8:\n",
    "        del news_all\n",
    "        gc.collect()\n",
    "        news_all = pd.DataFrame()\n",
    "        for item in news_pre:\n",
    "            news_all = pd.concat([news_all,item])\n",
    "        news_pre.clear()\n",
    "    \n",
    "#     index = news_all['time'][news_all['time'].dt.hour > 22].index\n",
    "#     news_all.loc[index,'time']  = news_all.loc[index,'time'].dt.ceil('d')\n",
    "    news_all['time'] = news_all['time'].dt.floor('d')\n",
    "    news_last = pd.DataFrame()\n",
    "    \n",
    "#     for col in cols:\n",
    "#         if col != 'relevance':\n",
    "#             print(col)\n",
    "#             news_all[col] = news_all[col] * news_all['relevance']\n",
    "    news_last = get_news_train(news_all)\n",
    "\n",
    "    market_obs_df['key'] = market_obs_df['time'].astype(str) + market_obs_df['assetName'].astype(str)\n",
    "    market_obs_df = market_obs_df.join(news_last,on = 'key',how='left')\n",
    "    \n",
    "    market_obs_df['close'].clip(upper = 1000, inplace = True)\n",
    "    market_obs_df['open'].clip(upper = 1000, inplace = True)\n",
    "    market_obs_df['volume'].clip(upper = 1e+08, inplace = True)\n",
    "    \n",
    "#     market_obs_df['close'] = np.log1p(market_obs_df['close'])\n",
    "#     market_obs_df['open'] = np.log1p(market_obs_df['open'])\n",
    "    \n",
    "#     col_mean = [num_cols].mean()\n",
    "    market_obs_df[num_cols]=market_obs_df[num_cols].fillna(col_mean)\n",
    "    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n",
    "    X_test = market_obs_df[num_cols]\n",
    "    X_test['assetCode'] = market_obs_df['assetCode'].apply(lambda x: encode(encoders[0], x)).values\n",
    "    X_test['assetName'] = market_obs_df['assetName'].apply(lambda x: encode(encoders[1], x)).values\n",
    "\n",
    "    \n",
    "    market_prediction = model.predict(X_test)*2 -1\n",
    "    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n",
    "\n",
    "    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n",
    "    # insert predictions to template\n",
    "    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n",
    "    env.predict(predictions_template_df)\n",
    "    del news_last\n",
    "    gc.collect()\n",
    "\n",
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0b10b1ee21cfa2ff48b7ad2b7351382c03daeaa"
   },
   "outputs": [],
   "source": [
    "# distribution of confidence as a sanity check: they should be distributed as above\n",
    "plt.hist(predicted_confidences, bins='auto')\n",
    "plt.title(\"predicted confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "512d64e04ad0533dd72ca6bcbca4882a7f289956"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
